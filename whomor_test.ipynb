{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# style_role_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusers-0.21.0.dev0\n",
    "# diffusers==0.21.3\n",
    "\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler,EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# prompt\n",
    "# style\n",
    "# prompt = \",1boy, medium hair, bangs, curly hair, orange hair, grey eyes, black none,\"\n",
    "# role\n",
    "prompt = \"1boy\"\n",
    "\n",
    "# lora_file\n",
    "# style\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_comic_style_lora_64_Lion_202311091733/whomor_comic_style_lora_64_Lion_202311091733-000024.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/korealove_0907_Lion_lora_64_best/korealove-000008.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/solo_leveling_0911_Lion_lora_64/solo_leveling_style-000008.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/superhero_comic_style_0907_Lion_lora_64/superhero_comic_style-000008.safetensors\"\n",
    "# role\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "\n",
    "# trigger words\n",
    "# style\n",
    "# trigger_words = [\"korea_comic_style\", \"solo_leveling_style\", \"superhero_comic_style\"]\n",
    "# role\n",
    "trigger_words = [\"whomor\"]\n",
    "\n",
    "base_model_path=\"/dfs/comicai/guanyu.zhao/models/model_0801_replicant\"\n",
    "pipe = DiffusionPipeline.from_pretrained(base_model_path, safety_checker=None, torch_dtype=torch.float16)\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# image = pipe(trigger_word+prompt,height=768,width=576,num_inference_steps=60,num_images_per_prompt=1).images[0]\n",
    "# image.save(\"./aaa.png\")\n",
    "for i, trigger_word in enumerate(trigger_words):\n",
    "    image = pipe(' '.join([prompt, trigger_word]),height=768,width=576,num_inference_steps=60,num_images_per_prompt=1).images[0]\n",
    "    image.save(f\"./output/style_role_lora/{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载方式1: 生成图发灰\n",
    "\n",
    "# diffusers-0.21.0.dev0\n",
    "# diffusers==0.21.3\n",
    "\n",
    "from diffusers import DiffusionPipeline, StableDiffusionXLPipeline, DPMSolverMultistepScheduler,EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# prompt\n",
    "# style\n",
    "# prompt = \",1boy, medium hair, bangs, curly hair, orange hair, grey eyes, black none,\"\n",
    "# role\n",
    "prompt = \"1boy\"\n",
    "\n",
    "# lora_file\n",
    "# style\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/stable-diffusion-webui/models/Lora/whomor_boy_style_sdxl_lora_64_Lion_202311161820-000012.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/korealove_0907_Lion_lora_64_best/korealove-000008.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/solo_leveling_0911_Lion_lora_64/solo_leveling_style-000008.safetensors\"\n",
    "# lora_file = \"/oss/comicai/guanyu.zhao/lora-scripts-main/output/superhero_comic_style_0907_Lion_lora_64/superhero_comic_style-000008.safetensors\"\n",
    "# role\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# trigger words\n",
    "# style\n",
    "# trigger_words=[\"olis style\"]\n",
    "# role\n",
    "trigger_words=[\"olis\"]\n",
    "\n",
    "base_model_path=\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "# 加载方式1：正常\n",
    "pipe = DiffusionPipeline.from_pretrained(base_model_path, safety_checker=None, torch_dtype=torch.float32)\n",
    "# 加载方式2：正常\n",
    "# pipe = StableDiffusionXLPipeline.from_pretrained(base_model_path, safety_checker=None, torch_dtype=torch.float32)\n",
    "\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# image = pipe(trigger_word+prompt,height=768,width=576,num_inference_steps=60,num_images_per_prompt=1).images[0]\n",
    "# image.save(\"./aaa.png\")\n",
    "for i, trigger_word in enumerate(trigger_words):\n",
    "    image = pipe(' '.join([prompt, trigger_word]),height=768,width=576,num_inference_steps=60,num_images_per_prompt=1).images[0]\n",
    "    image.save(f\"./output/style_role_lora/{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载方式2: guanyu.zhao建议，还没跑通，需下载模型\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        # Resize the image to fit the grid cell while maintaining aspect ratio\n",
    "        img.thumbnail((w, h), Image.ANTIALIAS)\n",
    "        \n",
    "        # Calculate position for placing the image in the grid\n",
    "        x_offset = (i % cols) * w\n",
    "        y_offset = (i // cols) * h\n",
    "        \n",
    "        # Paste the resized image onto the grid\n",
    "        grid.paste(img, (x_offset, y_offset))\n",
    "\n",
    "    return grid\n",
    "\n",
    "lora_role_file=\"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\" #countfit\n",
    "#lora_role_file=\"./output/whomor_boy_role_lora_64_Lion_202311161145/whomor_boy_role_lora_64_Lion_202311161145-000024.safetensors\"\n",
    "#lora_role_file=\"./output/whomor_boy_role_lora_64_Lion_202311161203/whomor_boy_role_lora_64_Lion_202311161203-000024.safetensors\"\n",
    "trigger_word = \"1boy olis solo\"\n",
    "prompt=\"\"\n",
    "#pipe = DiffusionPipeline.from_pretrained(base_model_path, torch_dtype=torch.float32)\n",
    "pipe=StableDiffusionXLPipeline.from_single_file(\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL/CounterfeitXL-V1.0.safetensors\")\n",
    "\n",
    "#generator = torch.Generator(\"cuda\").manual_seed(683096045)\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "print(pipe.tokenizer)\n",
    "pipe.load_lora_weights(lora_role_file)\n",
    "pipe.fuse_lora(lora_scale=1)\n",
    "pipe.to(\"cuda\")\n",
    "image = pipe(trigger_word+prompt,height=1024,width=1024,guidance_scale=7.5,num_inference_steps=30,num_images_per_prompt=1).images\n",
    "grid = image_grid(image, rows=1, cols=1)\n",
    "grid.save(\"./1115.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2I-Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD1.5 + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionAdapterPipeline, StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.pidi import PidiNetDetector\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2i-adapter-sketch-sdxl-1.0\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2iadapter_sketch_sd15v2\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "# model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model_id = \"/dfs/comicai/guanyu.zhao/models/model_0801_replicant\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "# vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "detector_path = \"/dfs/comicai/chenyu.liu/Models/lllyasviel_Annotators\" # \"lllyasviel/Annotators\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/org_sketch.png\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_2.PNG\"\n",
    "\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  adapter_path, torch_dtype=torch.float16, varient=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# load euler_a scheduler\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "# vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionAdapterPipeline.from_pretrained(\n",
    "    # model_id, vae=vae, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", \n",
    "    model_id, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", \n",
    ").to(device)\n",
    "\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_comic_style_lora_64_Lion_202311091733/whomor_comic_style_lora_64_Lion_202311091733-000024.safetensors\"\n",
    "# # lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "# pipe.load_lora_weights(lora_file)\n",
    "# pipe.to(device)\n",
    "\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pidinet = PidiNetDetector.from_pretrained(detector_path).to(\"cuda\")\n",
    "\n",
    "image = load_image(url)\n",
    "image = pidinet(\n",
    "  image, detect_resolution=1024, image_resolution=1024, apply_filter=True\n",
    ")\n",
    "image.save('./output/T2I-Adapter+LoRA/out_sketch_det.png')\n",
    "\n",
    "gen_images = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/T2I-Adapter+LoRA/out_sketch.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch-guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.pidi import PidiNetDetector\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2i-adapter-sketch-sdxl-1.0\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model_id = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "detector_path = \"/dfs/comicai/chenyu.liu/Models/lllyasviel_Annotators\" # \"lllyasviel/Annotators\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/org_sketch.png\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_2.PNG\"\n",
    "\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  adapter_path, torch_dtype=torch.float16, varient=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# load euler_a scheduler\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    model_id, vae=vae, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", \n",
    ").to(device)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pidinet = PidiNetDetector.from_pretrained(detector_path).to(\"cuda\")\n",
    "\n",
    "image = load_image(url)\n",
    "image = pidinet(\n",
    "  image, detect_resolution=1024, image_resolution=1024, apply_filter=True\n",
    ")\n",
    "image.save('./output/T2I-Adapter/out_sketch_det.png')\n",
    "\n",
    "gen_images = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/T2I-Adapter/out_sketch.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineart-guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.lineart import LineartDetector\n",
    "import torch\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  \"TencentARC/t2i-adapter-lineart-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# load euler_a scheduler\n",
    "model_id = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae=AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    model_id, vae=vae, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", \n",
    ").to(\"cuda\")\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "line_detector = LineartDetector.from_pretrained(\"lllyasviel/Annotators\").to(\"cuda\")\n",
    "\n",
    "url = \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_lin.jpg\"\n",
    "image = load_image(url)\n",
    "image = line_detector(\n",
    "    image, detect_resolution=384, image_resolution=1024\n",
    ")\n",
    "\n",
    "prompt = \"Ice dragon roar, 4k photo\"\n",
    "negative_prompt = \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
    "gen_images = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.8,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/T2I-Adapter/out_lin.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.pidi import PidiNetDetector\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2i-adapter-sketch-sdxl-1.0\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# model_id = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "detector_path = \"/dfs/comicai/chenyu.liu/Models/lllyasviel_Annotators\" # \"lllyasviel/Annotators\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/org_sketch.png\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_2.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/processed_images/01_whomor_layout/1280X1280_1.png\"\n",
    "\n",
    "# sd1.5 lora\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_comic_style_lora_64_Lion_202311091733/whomor_comic_style_lora_64_Lion_202311091733-000024.safetensors\"\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "# sdxl lora\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# civitai lora\n",
    "lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\" # Trigger Words: marvinmartian, alien\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/SDXL_MS_Paint_Portraits.safetensors\" # Trigger Words: MSPaint portrait, MSPaint drawing\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/VanGoghPortraiture_SD_XL_1.0.safetensors\" # Trigger Words: v0ng44g, p0rtr14t\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/InkPunk_XL.safetensors\" # Trigger Words: inkpunk\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Ivanov-Vano_Style_loRA_SDXL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/LineAniRedmond_Linear_Manga_Style_for_SD_XL_Anime_Style.safetensors\" # Trigger Words: lineart, LineAniAF\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Pixel_Art_XL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/TShirtDesignRedmond_T_Shirt_Design_Lora_for_SD_XL_1.0.safetensors\" # Trigger Words: T shirt design, TshirtDesignAF\n",
    "\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\", \"marvinmartian\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  adapter_path, torch_dtype=torch.float16, varient=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# load euler_a scheduler\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    model_id, vae=vae, adapter=adapter, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", \n",
    ").to(device)\n",
    "\n",
    "\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pidinet = PidiNetDetector.from_pretrained(detector_path).to(\"cuda\")\n",
    "\n",
    "image = load_image(url)\n",
    "image = pidinet(\n",
    "  image, detect_resolution=1024, image_resolution=1024, apply_filter=True\n",
    ")\n",
    "image.save('./output/Merge/T2I-Adapter+LoRA/out_sketch_det.png')\n",
    "\n",
    "gen_images = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/Merge/T2I-Adapter+LoRA/out_sketch.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 双重Pipeline加载，都可行，只有加载自训的lora不可行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.pidi import PidiNetDetector\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2i-adapter-sketch-sdxl-1.0\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# model_id = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "detector_path = \"/dfs/comicai/chenyu.liu/Models/lllyasviel_Annotators\" # \"lllyasviel/Annotators\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/org_sketch.png\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_2.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/processed_images/01_whomor_layout/1280X1280_1.png\"\n",
    "\n",
    "# sd1.5 lora\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_comic_style_lora_64_Lion_202311091733/whomor_comic_style_lora_64_Lion_202311091733-000024.safetensors\"\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "# sdxl lora\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# civitai lora\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\" # Trigger Words: marvinmartian, alien\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/SDXL_MS_Paint_Portraits.safetensors\" # Trigger Words: MSPaint portrait, MSPaint drawing\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/VanGoghPortraiture_SD_XL_1.0.safetensors\" # Trigger Words: v0ng44g, p0rtr14t\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/InkPunk_XL.safetensors\" # Trigger Words: inkpunk\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Ivanov-Vano_Style_loRA_SDXL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/LineAniRedmond_Linear_Manga_Style_for_SD_XL_Anime_Style.safetensors\" # Trigger Words: lineart, LineAniAF\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Pixel_Art_XL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/TShirtDesignRedmond_T_Shirt_Design_Lora_for_SD_XL_1.0.safetensors\" # Trigger Words: T shirt design, TshirtDesignAF\n",
    "\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\", \"v0ng44g\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  adapter_path, torch_dtype=torch.float16, varient=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# load euler_a scheduler\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id, vae=vae, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", # , adapter=adapter\n",
    ").to(device)\n",
    "\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "pipe_ = StableDiffusionXLAdapterPipeline(\n",
    "  vae=pipe.vae,\n",
    "  text_encoder=pipe.text_encoder,\n",
    "  text_encoder_2=pipe.text_encoder_2,\n",
    "  tokenizer=pipe.tokenizer,\n",
    "  tokenizer_2=pipe.tokenizer_2,\n",
    "  unet=pipe.unet,\n",
    "  adapter=adapter,\n",
    "  scheduler=pipe.scheduler,\n",
    "  force_zeros_for_empty_prompt=True,\n",
    ")\n",
    "\n",
    "pipe_.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pidinet = PidiNetDetector.from_pretrained(detector_path).to(\"cuda\")\n",
    "\n",
    "image = load_image(url)\n",
    "image = pidinet(\n",
    "  image, detect_resolution=1024, image_resolution=1024, apply_filter=True\n",
    ")\n",
    "image.save('./output/Merge/T2I-Adapter+LoRA/out_sketch_det.png')\n",
    "\n",
    "gen_images = pipe_(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/Merge/T2I-Adapter+LoRA/out_sketch.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 双重Pipeline加载，调试自训的lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from controlnet_aux.pidi import PidiNetDetector\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "adapter_path = \"/dfs/comicai/chenyu.liu/Models/TencentARC_t2i-adapter-sketch-sdxl-1.0\" # \"TencentARC/t2i-adapter-sketch-sdxl-1.0\"\n",
    "# model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model_id = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "detector_path = \"/dfs/comicai/chenyu.liu/Models/lllyasviel_Annotators\" # \"lllyasviel/Annotators\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/org_sketch.png\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_2.PNG\"\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/processed_images/01_whomor_layout/1280X1280_1.png\"\n",
    "\n",
    "# sd1.5 lora\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_comic_style_lora_64_Lion_202311091733/whomor_comic_style_lora_64_Lion_202311091733-000024.safetensors\"\n",
    "# x lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "# sdxl lora\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# civitai lora\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\" # Trigger Words: marvinmartian, alien\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/SDXL_MS_Paint_Portraits.safetensors\" # Trigger Words: MSPaint portrait, MSPaint drawing\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/VanGoghPortraiture_SD_XL_1.0.safetensors\" # Trigger Words: v0ng44g, p0rtr14t\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/InkPunk_XL.safetensors\" # Trigger Words: inkpunk\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Ivanov-Vano_Style_loRA_SDXL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/LineAniRedmond_Linear_Manga_Style_for_SD_XL_Anime_Style.safetensors\" # Trigger Words: lineart, LineAniAF\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/Pixel_Art_XL.safetensors\" # Trigger Words: style of ivanov vano, soviet cartoon\n",
    "# x lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/STYLE/TShirtDesignRedmond_T_Shirt_Design_Lora_for_SD_XL_1.0.safetensors\" # Trigger Words: T shirt design, TshirtDesignAF\n",
    "\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\", \"MSPaint portrait\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# load adapter\n",
    "adapter = T2IAdapter.from_pretrained(\n",
    "  adapter_path, torch_dtype=torch.float32#, varient=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# load euler_a scheduler\n",
    "# euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "# vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float32, #, vae=vae, scheduler=euler_a, variant=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "new_dtype = torch.float32\n",
    "pipe_ = StableDiffusionXLAdapterPipeline(\n",
    "  vae=pipe.vae,\n",
    "  text_encoder=pipe.text_encoder,\n",
    "  text_encoder_2=pipe.text_encoder_2,\n",
    "  tokenizer=pipe.tokenizer,\n",
    "  tokenizer_2=pipe.tokenizer_2,\n",
    "  unet=pipe.unet,\n",
    "  adapter=adapter,\n",
    "  scheduler=pipe.scheduler,\n",
    "  force_zeros_for_empty_prompt=True,\n",
    ")\n",
    "# pipe_.to(new_dtype)\n",
    "# pipe_.to(device)\n",
    "pipe_.to(new_dtype)\n",
    "\n",
    "\n",
    "pipe_.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pidinet = PidiNetDetector.from_pretrained(detector_path).to(\"cuda\")\n",
    "\n",
    "image = load_image(url)\n",
    "image = pidinet(\n",
    "  image, detect_resolution=1024, image_resolution=1024, apply_filter=True\n",
    ")\n",
    "image.save('./output/Merge/T2I-Adapter+LoRA/out_sketch_det.png')\n",
    "\n",
    "gen_images = pipe_(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    ").images[0]\n",
    "gen_images.save('./output/Merge/T2I-Adapter+LoRA/out_sketch.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet\n",
    "+ style_lora\n",
    "+ role_lora\n",
    "+ cannny\n",
    "+ prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lllyasviel/sd-controlnet-scribble\n",
    "https://huggingface.co/lllyasviel/sd-controlnet-scribble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import HEDdetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "# image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png\")\n",
    "\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/bag.png\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "image = load_image(url)\n",
    "\n",
    "# prompt = \"bag\"\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# image = hed(image, scribble=True)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/lllyasviel_sd-controlnet-scribble\", torch_dtype=torch.float16 #\"lllyasviel/sd-controlnet-scribble\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/Linaqruf_anything-v3.0\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16 # \"runwayml/stable-diffusion-v1-5\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Remove if you do not have xformers installed\n",
    "# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "# for installation instructions\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = pipe(prompt=prompt, negative_prompt=negative_prompt, image=image, num_inference_steps=20).images[0]\n",
    "\n",
    "image.save('./output/ControlNet/bag_scribble_out.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2：https://github.com/huggingface/diffusers/tree/v0.19.0/examples/controlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "base_model_path = \"/dfs/comicai/chenyu.liu/Models/Linaqruf_anything-v3.0\"\n",
    "controlnet_path = \"xx\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path, controlnet=controlnet, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# speed up diffusion process with faster scheduler and memory optimization\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# remove following line if xformers is not installed or when using Torch 2.0.\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "# memory optimization.\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "control_image = load_image(\"./conditioning_image_1.png\")\n",
    "prompt = \"pale golden rod circle with old lace background\"\n",
    "\n",
    "# generate image\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=20, generator=generator, image=control_image\n",
    ").images[0]\n",
    "image.save(\"./output/ControlNet/conditioning_image_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD1.5 + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://huggingface.co/lllyasviel/sd-controlnet-scribble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import HEDdetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# hed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "# image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png\")\n",
    "\n",
    "# url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/bag.png\"\n",
    "url = \"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\"\n",
    "image = load_image(url)\n",
    "\n",
    "# prompt = \"bag\"\n",
    "# prompt = \"a robot, mount fuji in the background, 4k photo, highly detailed\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# prompt = \"dragon,big wing,blue skin,dark blue background,midnight, aftermath of a late night, the autumn air was fresh, the chill is striking, \"\n",
    "other_prompts = \", \".join([\"anime, 4k photo, high quality, highly detailed\", \"sots art,Alvan Fisher,official art\"])\n",
    "prompt = prompt + other_prompts\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "# negative_prompt = \"nsfw,lowres,bad anatomy,bad hands,signature,watermark,username\"\n",
    "\n",
    "\n",
    "# image = hed(image, scribble=True)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/lllyasviel_sd-controlnet-scribble\", torch_dtype=torch.float16 #\"lllyasviel/sd-controlnet-scribble\"\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/Linaqruf_anything-v3.0\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16 # \"runwayml/stable-diffusion-v1-5\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "lora_file = \"\"\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "# Remove if you do not have xformers installed\n",
    "# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "# for installation instructions\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = pipe(prompt=prompt, negative_prompt=negative_prompt, image=image, num_inference_steps=20).images[0]\n",
    "\n",
    "image.save('./output/ControlNet/bag_scribble_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lllyasviel/sd-controlnet-canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png\")\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Remove if you do not have xformers installed\n",
    "# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n",
    "# for installation instructions\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = pipe(\"bird\", image, num_inference_steps=20).images[0]\n",
    "\n",
    "image.save('images/bird_canny_out.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diffusers/controlnet-canny-sdxl-1.0\n",
    "https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# negative_prompt = 'low quality, bad quality, sketches'\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "# image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/hf-logo.png\")\n",
    "image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\")\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\", # \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\", # \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "images = pipe(\n",
    "    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    ).images\n",
    "\n",
    "images[0].save(f\"./output/ControlNet/hug_lab.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通用\n",
    "失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# negative_prompt = 'low quality, bad quality, sketches'\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "# image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/hf-logo.png\")\n",
    "image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\")\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\", # \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\", # \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/stable-diffusion-webui/models/Lora/whomor_boy_style_sdxl_lora_64_Lion_202311161820-000012.safetensors\"\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "images = pipe(\n",
    "    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    ).images\n",
    "\n",
    "images[0].save(f\"./output/ControlNet/hug_lab.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双重Pipeline加载\n",
    "成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionXLPipeline, StableDiffusionXLControlNetPipeline, AutoencoderKL, EulerAncestralDiscreteScheduler\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "prompt = \"1boy,bangs,black hair,blue eyes,cloud,hair between eyes,jacket,male focus,short hair,solo,upper body, wearing a cape and caped outfit with a red cape\"\n",
    "# negative_prompt = 'low quality, bad quality, sketches'\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "# image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/hf-logo.png\")\n",
    "image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout/1280X1280_1.PNG\")\n",
    "# image = load_image(\"/dfs/comicai/chenyu.liu/Whomor/images_test/02_whomor_line/1280X1280_1.PNG\")\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\", # \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model_id = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\"\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id, vae=vae, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", # , adapter=adapter\n",
    ").to(device)\n",
    "\n",
    "# style lora\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/stable-diffusion-webui/models/Lora/whomor_boy_style_sdxl_lora_64_Lion_202311161820-000012.safetensors\"\n",
    "# role lora\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "pipe_ = StableDiffusionXLControlNetPipeline(\n",
    "    vae=pipe.vae,\n",
    "    text_encoder=pipe.text_encoder,\n",
    "    text_encoder_2=pipe.text_encoder_2,\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    tokenizer_2=pipe.tokenizer_2,\n",
    "    unet=pipe.unet,\n",
    "    controlnet=controlnet,\n",
    "    scheduler=pipe.scheduler,\n",
    ")\n",
    "\n",
    "# pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "#     \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\", # \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     controlnet=controlnet,\n",
    "#     vae=vae,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "pipe_.enable_model_cpu_offload()\n",
    "\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "images = pipe_(\n",
    "    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    ).images\n",
    "\n",
    "images[0].save(f\"./output/ControlNet/hug_lab.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP-Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base depth\n",
    "按照源码修改，由于只下载了controlnet_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\"，与输入的条件图深度图depth_map不匹配，所以结果不符合，此处只为了测试能跑通，别无他用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, StableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapterXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/image_encoder\" # \"models/image_encoder\"\n",
    "ip_ckpt = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/sdxl_models/ip-adapter_sdxl_vit-h.bin\" # \"models/ip-adapter_sdxl_vit-h.bin\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe, ip_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "controlnet_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\" # \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "# load SDXL pipeline\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    add_watermarker=False,\n",
    ").to(device)\n",
    "\n",
    "# load ip-adapter\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# read image prompt\n",
    "image = Image.open(\"assets/images/statue.png\")\n",
    "depth_map = Image.open(\"assets/structure_controls/depth.png\").resize((1024, 1024))\n",
    "image_grid([image.resize((256, 256)), depth_map.resize((256, 256))], 1, 2)\n",
    "\n",
    "# generate image with structural control\n",
    "num_samples = 2\n",
    "images = ip_model.generate(pil_image=image, image=depth_map, controlnet_conditioning_scale=0.7, num_samples=num_samples, num_inference_steps=30, seed=42)\n",
    "grid = image_grid(images, 1, num_samples)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal prompts\n",
    "images = ip_model.generate(pil_image=image, num_samples=num_samples, num_inference_steps=30, seed=420,\n",
    "        prompt=\"best quality, high quality, wearing sunglasses on the beach\", scale=0.6)\n",
    "grid = image_grid(images, 1, num_samples)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mine SDXL+unfine_tuned IP-Adapter\n",
    "sdxl1.0 + canny\n",
    "画风没有适应whomor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, StableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapterXL\n",
    "\n",
    "\n",
    "base_model_path = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/image_encoder\" # \"models/image_encoder\"\n",
    "ip_ckpt = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/sdxl_models/ip-adapter_sdxl_vit-h.bin\" # \"models/ip-adapter_sdxl_vit-h.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "# del pipe, ip_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "controlnet_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\" # \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "# load SDXL pipeline\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    add_watermarker=False,\n",
    ").to(device)\n",
    "\n",
    "# lora_file= \"/dfs/comicai/guanyu.zhao/stable-diffusion-webui/models/Lora/whomor_boy_style_sdxl_lora_64_Lion_202311161820-000012.safetensors\"\n",
    "# # lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\"\n",
    "# pipe.load_lora_weights(lora_file)\n",
    "# pipe.to(device)\n",
    "\n",
    "# load ip-adapter\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image prompt\n",
    "image = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/ori/boy_1.png\") # \"/dfs/comicai/chenyu.liu/Whomor/ori/boy2_1024_1024.png\"\n",
    "canny_map = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/images_test/01_whomor_layout_canny/1280X1280_1.PNG\") # .resize((1024, 1024))\n",
    "# image_grid([image.resize((256, 256)), canny_map.resize((256, 256))], 1, 2)\n",
    "image_grid([image, canny_map], 1, 2)\n",
    "\n",
    "# generate image with structural control\n",
    "num_samples = 2\n",
    "images = ip_model.generate(pil_image=image, image=canny_map, controlnet_conditioning_scale=0.7, num_samples=num_samples, num_inference_steps=30, seed=42)\n",
    "grid = image_grid(images, 1, num_samples)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "model_0801 (sd1.5) + fine-tuned control_v11p_sd15_canny\n",
    "画风更接近whomor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, DDIMScheduler, AutoencoderKL, ControlNetModel\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapter\n",
    "\n",
    "# base_model_path = \"/dfs/comicai/zhiyuan.shi/models/stable-diffusion-v1-5\"\n",
    "# image_encoder_path = \"models/image_encoder/\"\n",
    "# ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "\n",
    "base_model_path = \"/dfs/comicai/zhiyuan.shi/models/production_models/model_0801\"\n",
    "image_encoder_path = \"/dfs/comicai/zhiyuan.shi/models/IP-Adapter/models/image_encoder/\"\n",
    "# ip_ckpt = \"/dfs/comicai/zhiyuan.shi/models/IP-Adapter/models/ip-adapter_sd15.bin\"\n",
    "nums = 52000\n",
    "ip_ckpt = \"/dfs/comicai/yang.zhang/IP-Adapter/models_train_ckpt/ip_adapter_{}.bin\".format(nums)\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "        # grid.paste(img, box=(i%cols*w, i//cols*h, (i%cols + 1)*w, (i//cols + 1)*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "# load controlnet\n",
    "controlnet_model_path = \"/dfs/comicai/zhiyuan.shi/models/control_v11p_sd15_canny\"\n",
    "# controlnet_model_path = \"/dfs/comicai/zhiyuan.shi/models/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/ori/111.png\")\n",
    "# depth_map = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/ori/222.png\")\n",
    "\n",
    "image = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/batch/person/0.png\")\n",
    "depth_map = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/batch/canny/1.png\")\n",
    "# depth_map = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/canny_output_train_controlnet_finetune_scribble.png\")\n",
    "\n",
    "images = ip_model.generate(pil_image=image, image=depth_map, num_samples=1, num_inference_steps=50, seed=42)\n",
    "\n",
    "grid = image_grid(images, 1, 1)\n",
    "grid\n",
    "\n",
    "# # img_num = \"8\"\n",
    "# for j in range(1,2):\n",
    "#     blue = j\n",
    "#     for i in range(1,4):\n",
    "#         role = \"girl_boy{}\".format(i)\n",
    "#         assets_dir = \"/home/yang.zhang/code/stable_diffusion_reference_only/assets/\"\n",
    "#         # read image prompt\n",
    "#         # image = Image.open(assets_dir + \"{}_prompt.png\".format(img_num))\n",
    "#         image = Image.open(\"/home/yang.zhang/code/IP-Adapter/structural/{}.jpg\".format(role))\n",
    "#         # depth_map = Image.open(assets_dir + \"{}_blueprint.png\".format(img_num))\n",
    "#         depth_map = Image.open(\"/home/yang.zhang/code/IP-Adapter/structural/blue{}.png\".format(blue))\n",
    "#         # result_img = Image.open(assets_dir + \"{}_result.png\".format(img_num))\n",
    "#         image_grid([image.resize((256, 256)), depth_map.resize((256, 256))], 1, 2)\n",
    "\n",
    "#         # load ip-adapter\n",
    "#         ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "#         # generate image variations\n",
    "#         images = ip_model.generate(pil_image=image, image=depth_map, num_samples=1, num_inference_steps=50, seed=42)\n",
    "#         # img_list = [image.resize((256, 256)),depth_map.resize((256, 256)),result_img.resize((256, 256)),images]\n",
    "#         # grid = image_grid([image.resize((512, 512)), depth_map.resize((512, 512)), result_img.resize((512, 512)), images], 1, 4)\n",
    "#         grid = image_grid(images, 1, 1)\n",
    "#         grid\n",
    "#         # file_path = os.path.join(save_dir,\"img_{11}\".format())\n",
    "\n",
    "#         file_path = \"/dfs/comicai/chenyu.liu/Whomor/output/IP-Adapter/structural/result_{}_blue{}_{}.jpg\".format(role,blue,nums)\n",
    "#         grid.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch\n",
    "最接近线上Tab\"IP-Adapter\"demo的代码\n",
    "\n",
    "批量操作：\n",
    "1. 有sdxl1.0 + canny\n",
    "2. 有model_0801 (sd1.5) + fine-tuned control_v11p_sd15_canny\n",
    "\n",
    "测试结果见：https://rg975ojk5z.feishu.cn/sheets/GNOis84zsh0DvXtev73cOW1Jnbc?sheet=J6C3rf&range=MToxOTY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71379b478a304703b116ca2c7eb9bb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, StableDiffusionControlNetPipeline, DDIMScheduler, AutoencoderKL, ControlNetModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ip_adapter import IPAdapter, IPAdapterXL\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# mine SDXL+unfine_tuned IP-Adapter\n",
    "base_model_path = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/image_encoder\" # \"models/image_encoder\"\n",
    "# ip_ckpt = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/sdxl_models/ip-adapter_sdxl_vit-h.bin\" # 没有经过微调的\n",
    "image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/sdxl_models/image_encoder\" # \"models/image_encoder\"\n",
    "ip_ckpt = \"/dfs/comicai/yang.zhang/IP-Adapter/models_trainsdxl_ckpt/ip_adapter_sdxl480000.bin\" # 经过微调的\n",
    "controlnet_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\" # \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "# zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "# base_model_path = \"/dfs/comicai/zhiyuan.shi/models/production_models/model_0801\"\n",
    "# image_encoder_path = \"/dfs/comicai/zhiyuan.shi/models/IP-Adapter/models/image_encoder/\"\n",
    "# ip_ckpt = \"/dfs/comicai/zhiyuan.shi/models/IP-Adapter/models/ip-adapter_sd15.bin\" # 没有经过微调的\n",
    "# # ip_ckpt = \"/dfs/comicai/yang.zhang/IP-Adapter/models_train_ckpt/ip_adapter_52000.bin\" # 经过微调的\n",
    "# controlnet_path = \"/dfs/comicai/zhiyuan.shi/models/control_v11p_sd15_canny\"\n",
    "\n",
    "# load controlnet\n",
    "# mine SDXL+unfine_tuned IP-Adapter\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "# zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "# controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "# mine SDXL+unfine_tuned IP-Adapter\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    add_watermarker=False,\n",
    ").to(device)\n",
    "# zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "# pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "#     base_model_path,\n",
    "#     controlnet=controlnet,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     feature_extractor=None,\n",
    "#     safety_checker=None\n",
    "# )\n",
    "\n",
    "# mine SDXL+unfine_tuned IP-Adapter\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n",
    "# zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "# ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309c8cfb1e3e41d28c8bd910903811f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b13f28216a463ea44c3ff419614487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc783b8eaac4254a629112eafa24a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d03e3354abb4ee9a1c2a343ddc2d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1f21a9515b4cd5b15b18bfe77f5ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31603c2cef0d401d827577d0e0fbf3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469c91ce9c70400cb4602c0e3955afa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e203fe09fd0240db96c65407b7f2204c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077e40d50d534bf38becbcc21b1684fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49671dd3b19743c6aff8c23d2e0e4e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8215e1ec9fa249d8a47cb70f5e7357b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68af09e2ca534680b510f3e68127097a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f48d7049f0480eab54c2e99633afe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f5148de3054176b7e3b3e4036c3175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d281f4e1a9dc46b68cb2fa79272206a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd01649671374fb1a73755180a0e6b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb15874ac9b4716a8fcd78d25f820d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37b14bd4e7a44618f7c34a7b3ea3c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10083f7506e9453591b618f4a57ac129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3907621aa0df43ed8c0fbca9ddfa44ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d56db73700499fa01b556c612a408d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de12911cd2c7487a8c49309d6749b802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b448233ebc4bf98f2d2d1ae7f04e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee611062fde45b2a9e2608c2d79b277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f19d94f6e946308b35265760e2ce96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea6a4c9582e4341906749103c855fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9d53b0495047e583d5f4d10e066c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917b018e5dfd402e927d0618bfde29ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4909c2320928432d9524318b7f50d433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0344f097fb964d54a963f69c582b71c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        # Resize the image to fit the grid cell while maintaining aspect ratio\n",
    "        img.thumbnail((w, h)) # , Image.ANTIALIAS\n",
    "        \n",
    "        # Calculate position for placing the image in the grid\n",
    "        x_offset = (i % cols) * w\n",
    "        y_offset = (i // cols) * h\n",
    "        \n",
    "        # Paste the resized image onto the grid\n",
    "        grid.paste(img, (x_offset, y_offset))\n",
    "\n",
    "    return grid\n",
    "\n",
    "# person_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/person\"\n",
    "person_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/person_white_bg\"\n",
    "line_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/line\"\n",
    "# line_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/color\"\n",
    "canny_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/canny\"\n",
    "out_path = \"/dfs/comicai/chenyu.liu/Whomor/batch/output\"\n",
    "person_files = os.listdir(person_path)\n",
    "line_files = os.listdir(line_path)\n",
    "person_files = sorted([person_file for person_file in person_files if person_file not in ['.ipynb_checkpoints']])\n",
    "line_files = sorted([line_file for line_file in line_files if line_file not in ['.ipynb_checkpoints']])\n",
    "\n",
    "for i, person_file in enumerate(person_files): # [0:1]\n",
    "    output_list = []\n",
    "    person_image = Image.open(os.path.join(person_path, person_file))\n",
    "    \n",
    "    for j, line_file in enumerate(line_files): # [0:3]\n",
    "        line_image = cv2.imread(os.path.join(line_path, line_file), cv2.IMREAD_GRAYSCALE)\n",
    "        line_canny_image = cv2.Canny(line_image, threshold1=100, threshold2=200)\n",
    "        line_canny_image = Image.fromarray(line_canny_image)\n",
    "        line_canny_image.save(f\"{canny_path}/{line_file}\")\n",
    "\n",
    "        # mine SDXL+unfine_tuned IP-Adapter\n",
    "        images = ip_model.generate(pil_image=person_image, image=line_canny_image, controlnet_conditioning_scale=0.7, num_samples=1, num_inference_steps=30, seed=42)\n",
    "        # zhangyang SD1.5+fine_tuned IP-Adapter\n",
    "        # images = ip_model.generate(pil_image=person_image, image=line_canny_image, num_samples=1, num_inference_steps=50, seed=42)\n",
    "        output_list.append(images[0])\n",
    "    grid = image_grid(output_list, 1, len(output_list))\n",
    "    grid.save(f\"{out_path}/person_{person_file.split('.')[0]}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan 1\n",
    "\n",
    "1. UNet load condition \"style lora\"\n",
    "2. use IP-Adapter\n",
    "- Image Prompt: condition \"role image\"\n",
    "- Structural Controls: scibble image\n",
    "\n",
    "失败：AttributeError: 'UNet2DConditionModel' object has no attribute 'attn_processors'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapterXL\n",
    "\n",
    "\n",
    "base_model_path = \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\" # \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/image_encoder\" # \"models/image_encoder\"\n",
    "ip_ckpt = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/sdxl_models/ip-adapter_sdxl_vit-h.bin\" # \"models/ip-adapter_sdxl_vit-h.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "# del pipe, ip_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load euler_a scheduler\n",
    "euler_a = EulerAncestralDiscreteScheduler.from_pretrained(base_model_path, subfolder=\"scheduler\")\n",
    "vae_path = \"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\" # \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "vae=AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    base_model_path, vae=vae, scheduler=euler_a, torch_dtype=torch.float16, variant=\"fp16\", # , adapter=adapter\n",
    ").to(device)\n",
    "\n",
    "lora_file= \"/dfs/comicai/guanyu.zhao/stable-diffusion-webui/models/Lora/whomor_boy_style_sdxl_lora_64_Lion_202311161820-000012.safetensors\"\n",
    "lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\"\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(device)\n",
    "\n",
    "controlnet_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_controlnet-canny-sdxl-1.0\" # \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "# load SDXL pipeline\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "# pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "#     base_model_path,\n",
    "#     controlnet=controlnet,\n",
    "#     use_safetensors=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     add_watermarker=False,\n",
    "# ).to(device)\n",
    "pipe_ = StableDiffusionXLControlNetPipeline(\n",
    "    vae=pipe.vae,\n",
    "    text_encoder=pipe.text_encoder,\n",
    "    text_encoder_2=pipe.text_encoder_2,\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    tokenizer_2=pipe.tokenizer_2,\n",
    "    unet=pipe.unet,\n",
    "    controlnet=controlnet,\n",
    "    scheduler=pipe.scheduler,\n",
    ")\n",
    "\n",
    "# load ip-adapter\n",
    "ip_model = IPAdapterXL(pipe_, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# read image prompt\n",
    "image = Image.open(\"assets/images/statue.png\")\n",
    "depth_map = Image.open(\"assets/structure_controls/depth.png\").resize((1024, 1024))\n",
    "image_grid([image.resize((256, 256)), depth_map.resize((256, 256))], 1, 2)\n",
    "\n",
    "# generate image with structural control\n",
    "num_samples = 2\n",
    "images = ip_model.generate(pil_image=image, image=depth_map, controlnet_conditioning_scale=0.7, num_samples=num_samples, num_inference_steps=30, seed=42)\n",
    "grid = image_grid(images, 1, num_samples)\n",
    "grid\n",
    "\n",
    "del pipe\n",
    "del pipe_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapter\n",
    "\n",
    "\n",
    "# base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "# vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "# image_encoder_path = \"models/image_encoder/\"\n",
    "# ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "\n",
    "base_model_path = \"/dfs/comicai/chenyu.liu/Models/Linaqruf_anything-v3.0\"\n",
    "vae_model_path = \"/dfs/comicai/chenyu.liu/Models/stabilityai_sd-vae-ft-mse\"\n",
    "image_encoder_path = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/image_encoder\"\n",
    "ip_ckpt = \"/dfs/comicai/chenyu.liu/Models/h94_IP-Adapter/models/ip-adapter_sd15.bin\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD Inpainting pipe\n",
    "# del pipe, ip_model\n",
    "torch.cuda.empty_cache()\n",
    "pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "# load ip-adapter\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read image prompt\n",
    "# image = Image.open(\"assets/images/girl.png\")\n",
    "# image.resize((256, 256))\n",
    "\n",
    "# read image prompt\n",
    "image = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\")\n",
    "image\n",
    "\n",
    "# masked_image = Image.open(\"assets/inpainting/image.png\").resize((512, 768))\n",
    "# mask = Image.open(\"assets/inpainting/mask.png\").resize((512, 768))\n",
    "\n",
    "masked_image = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/0.png\").convert(\"RGB\") # assets/inpainting/image.png\n",
    "mask = Image.open(\"/dfs/comicai/chenyu.liu/Whomor/2.png\").convert(\"RGB\") # assets/inpainting/mask.png\n",
    "print(masked_image.mode,mask.mode)\n",
    "image_grid([masked_image, mask], 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "images = ip_model.generate(pil_image=image, num_samples=4, num_inference_steps=50,\n",
    "                           seed=42, image=masked_image, mask_image=mask, strength=0.7, )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tune scribble test\n",
    "1. fine-tune scribble (by whomor layout-color): 运行train_controlnet_data_process.ipynb/Train SD1.5/for finetune ControlNet scribble SD1.5\n",
    "2. generate image: 运行train_controlnet_data_process.ipynb/Test SD1.5/for finetune ControlNet scribble SD1.5\n",
    "3. generate canny: 运行test.ipynb/Image processing/Extract image canny\n",
    "4. fine-tune IPAdapter (by whomor color, get style_lora): 运行test.ipynb/IP-Adapter/batch\n",
    "    1. 测试了原始的IPAdapter\n",
    "    2. SDXL的IPAdapter，待微调，下周问一下张洋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load multiple LoRAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multiple LoRAs\n",
    "https://github.com/huggingface/diffusers/blob/ff573ae245166d8b958950f68eb65ea766f07f69/docs/source/en/using-diffusers/loading_adapters.md\n",
    "diffusers>=0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, AutoencoderKL\n",
    "import torch\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"/dfs/comicai/algo-assemble-platform/models/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_lora_weights(\"/dfs/comicai/chenyu.liu/Models/ostris_ikea-instructions-lora-sdxl/ikea_instructions_xl_v1_5.safetensors\")\n",
    "pipeline.fuse_lora(lora_scale=0.7)\n",
    "\n",
    "# to unfuse the LoRA weights\n",
    "# pipeline.unfuse_lora()\n",
    "\n",
    "pipeline.load_lora_weights(\"/dfs/comicai/chenyu.liu/Models/ostris_super-cereal-sdxl-lora/cereal_box_sdxl_v1.safetensors\")\n",
    "pipeline.fuse_lora(lora_scale=0.7)\n",
    "\n",
    "prompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\n",
    "image = pipeline(prompt).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LoRAs for inference\n",
    "https://huggingface.co/docs/diffusers/main/en/tutorials/using_peft_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/diffusers/blob/7d6f30e89ba3460dd26235c298c54d2ddb9d1590/docs/source/en/api/loaders/lora.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 场景库（背景+人物）Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask + prompt\n",
    "from wei.wang\n",
    "\n",
    "diffusers==0.24.0\n",
    "\n",
    "目前最优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLInpaintPipeline\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "seed = random.randint(0, 10000)\n",
    "seed = 8344\n",
    "print(seed)\n",
    "height, width = 1024, 1024\n",
    "num_inference_steps = 30\n",
    "# model_path = '/home/wei.wang/checkpoint/stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "model_path = '/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "\n",
    "# loading model\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "model = StableDiffusionXLInpaintPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\n",
    "# load canvas\n",
    "# canvas = Image.open('assert/3.png')\n",
    "canvas = Image.open('/dfs/comicai/chenyu.liu/Whomor/ori/1280X1280.PNG')\n",
    "\n",
    "\n",
    "# # generate character\n",
    "with torch.no_grad():\n",
    "    prompt = '1girl, long hair, curly hair, orange hair, black eyes, green tank-tops, sleeveless tank-tops, brown pants, long pants, green earrings, green hair ribbon, standing'\n",
    "    neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask[200:, :300] = 1\n",
    "    mask_image = Image.fromarray(np.stack([mask * 255, mask * 255, mask * 255], axis=-1), 'RGB')\n",
    "    latents = model(prompt, image=canvas, mask_image=mask, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    prompt = '1boy, medium hair, bangs, yellow hair, red suit jacket, black pants, yellow glasses, standing'\n",
    "    neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 400:700] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "    prompt = '1girl, very long hair, bangs, curly hair, yellow hair, grey eyes, yellow tank-tops, black skirt, short skirt, silver earrings, standing'\n",
    "    neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 800:1100] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "    prompt = '1boy, short hair, bangs, black hair, black eyes, business suit, white shirt, black suit jacket, black bow tie, standing'\n",
    "    neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 1200:] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "    image = model.vae.decode(latents / model.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = model.image_processor.postprocess(image, output_type='pil')[0]\n",
    "\n",
    "image.save('out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask + LoRA\n",
    "\n",
    "目前最优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLInpaintPipeline, AutoencoderKL\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "seed = random.randint(0, 10000)\n",
    "seed = 8344\n",
    "print(seed)\n",
    "height, width = 1024, 1024\n",
    "num_inference_steps = 30\n",
    "# model_path = '/home/wei.wang/checkpoint/stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "model_path = '/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "\n",
    "# loading model\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "model = StableDiffusionXLInpaintPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "# vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "# model.vae = vae\n",
    "\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "# promtp_ = '1boy olis solo'\n",
    "lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\" # Trigger Words: marvinmartian, alien\n",
    "promtp_ = \"marvinmartian, alien\"\n",
    "neg_prompt_ = \"\"\n",
    "\n",
    "model.load_lora_weights(lora_file)\n",
    "model.to(device)\n",
    "\n",
    "# load canvas\n",
    "# canvas = Image.open('assert/3.png')\n",
    "canvas = Image.open('/dfs/comicai/chenyu.liu/Whomor/ori/1280X1280.PNG')\n",
    "\n",
    "# # generate character\n",
    "with torch.no_grad():\n",
    "    # prompt = '1girl, long hair, curly hair, orange hair, black eyes, green tank-tops, sleeveless tank-tops, brown pants, long pants, green earrings, green hair ribbon, standing'\n",
    "    # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    prompt = promtp_\n",
    "    neg_prompt = neg_prompt_\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask[200:, :300] = 1\n",
    "    mask_image = Image.fromarray(np.stack([mask * 255, mask * 255, mask * 255], axis=-1), 'RGB')\n",
    "    latents = model(prompt, image=canvas, mask_image=mask, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    # prompt = '1boy, medium hair, bangs, yellow hair, red suit jacket, black pants, yellow glasses, standing'\n",
    "    # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    prompt = promtp_\n",
    "    neg_prompt = neg_prompt_\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 400:700] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "    # prompt = '1girl, very long hair, bangs, curly hair, yellow hair, grey eyes, yellow tank-tops, black skirt, short skirt, silver earrings, standing'\n",
    "    # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    prompt = promtp_\n",
    "    neg_prompt = neg_prompt_\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 800:1100] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "    # prompt = '1boy, short hair, bangs, black hair, black eyes, business suit, white shirt, black suit jacket, black bow tie, standing'\n",
    "    # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    prompt = promtp_\n",
    "    neg_prompt = neg_prompt_\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:, 1200:] = 1\n",
    "\n",
    "    latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    init_image = init_image.to(dtype=torch.float32)\n",
    "    mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    masked_image = init_image * (mask < 0.5)\n",
    "    mask, masked_image_latents = model.prepare_mask_latents(\n",
    "                mask,\n",
    "                masked_image,\n",
    "                1,\n",
    "                height,\n",
    "                width,\n",
    "                torch.float16,\n",
    "                device,\n",
    "                torch.manual_seed(666),\n",
    "                True,\n",
    "    )\n",
    "    init_mask = mask[:1]\n",
    "    latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "    image = model.vae.decode(latents / model.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = model.image_processor.postprocess(image, output_type='pil')[0]\n",
    "\n",
    "image.save('output/temp/inpainting.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "1. vae颜色\n",
    "2. 角色lora一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLInpaintPipeline, AutoencoderKL\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "device = 'cuda:1'\n",
    "seed = random.randint(0, 10000)\n",
    "seed = 8344\n",
    "print(seed)\n",
    "height, width = 1024, 400\n",
    "num_inference_steps = 30\n",
    "# model_path = '/home/wei.wang/checkpoint/stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "model_path = '/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1'\n",
    "\n",
    "# loading model\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "model = StableDiffusionXLInpaintPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(device)\n",
    "# vae = AutoencoderKL.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/madebyollin_sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "# model.vae = vae\n",
    "\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "promtp_ = '1boy olis solo'\n",
    "# lora_file = \"/dfs/comicai/chenyu.liu/Models/LoRAs/ROLE/Marvin_the_Martian.safetensors\" # Trigger Words: marvinmartian, alien\n",
    "# promtp_ = \"marvinmartian, alien\"\n",
    "neg_prompt_ = \"\"\n",
    "\n",
    "model.load_lora_weights(lora_file)\n",
    "model.to(device)\n",
    "\n",
    "# load canvas\n",
    "# canvas = Image.open('assert/3.png')\n",
    "canvas = Image.open('/dfs/comicai/chenyu.liu/Whomor/ori/1024X400.PNG')\n",
    "# canvas_ori = Image.open('/dfs/comicai/chenyu.liu/Whomor/ori/1280X1280.PNG')\n",
    "\n",
    "# # generate character\n",
    "with torch.no_grad():\n",
    "    # prompt = '1girl, long hair, curly hair, orange hair, black eyes, green tank-tops, sleeveless tank-tops, brown pants, long pants, green earrings, green hair ribbon, standing'\n",
    "    # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    prompt = promtp_\n",
    "    neg_prompt = neg_prompt_\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[200:600, :300] = 1\n",
    "    # mask_image[:, :] = 1\n",
    "    # 定义左上角和右下角的坐标\n",
    "    # left_upper = (0, 0)\n",
    "    # right_lower = (500, 1024)\n",
    "    # left_upper = (100, 0)\n",
    "    # right_lower = (400, 1024)\n",
    "    # canvas = canvas_ori.crop((*left_upper, *right_lower))\n",
    "    # mask_image = Image.fromarray(np.stack([mask * 255, mask * 255, mask * 255], axis=-1), 'RGB')\n",
    "    latents = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # prompt = '1boy, medium hair, bangs, yellow hair, red suit jacket, black pants, yellow glasses, standing'\n",
    "    # # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    # prompt = promtp_\n",
    "    # neg_prompt = neg_prompt_\n",
    "    # mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    # mask_image[200:, 400:700] = 1\n",
    "\n",
    "    # latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    # init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    # init_image = init_image.to(dtype=torch.float32)\n",
    "    # mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    # masked_image = init_image * (mask < 0.5)\n",
    "    # mask, masked_image_latents = model.prepare_mask_latents(\n",
    "    #             mask,\n",
    "    #             masked_image,\n",
    "    #             1,\n",
    "    #             height,\n",
    "    #             width,\n",
    "    #             torch.float16,\n",
    "    #             device,\n",
    "    #             torch.manual_seed(666),\n",
    "    #             True,\n",
    "    # )\n",
    "    # init_mask = mask[:1]\n",
    "    # latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # prompt = '1girl, very long hair, bangs, curly hair, yellow hair, grey eyes, yellow tank-tops, black skirt, short skirt, silver earrings, standing'\n",
    "    # # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    # prompt = promtp_\n",
    "    # neg_prompt = neg_prompt_\n",
    "    # mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    # mask_image[200:, 800:1100] = 1\n",
    "\n",
    "    # latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    # init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    # init_image = init_image.to(dtype=torch.float32)\n",
    "    # mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    # masked_image = init_image * (mask < 0.5)\n",
    "    # mask, masked_image_latents = model.prepare_mask_latents(\n",
    "    #             mask,\n",
    "    #             masked_image,\n",
    "    #             1,\n",
    "    #             height,\n",
    "    #             width,\n",
    "    #             torch.float16,\n",
    "    #             device,\n",
    "    #             torch.manual_seed(666),\n",
    "    #             True,\n",
    "    # )\n",
    "    # init_mask = mask[:1]\n",
    "    # latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # prompt = '1boy, short hair, bangs, black hair, black eyes, business suit, white shirt, black suit jacket, black bow tie, standing'\n",
    "    # # neg_prompt = 'painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured'\n",
    "    # prompt = promtp_\n",
    "    # neg_prompt = neg_prompt_\n",
    "    # mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    # mask_image[200:, 1200:] = 1\n",
    "\n",
    "    # latent = model(prompt, image=canvas, mask_image=mask_image, negative_prompt=neg_prompt, width=width, height=height, num_inference_steps=num_inference_steps, strength=1, output_type='latent', generator=torch.manual_seed(seed)).images\n",
    "\n",
    "    # init_image = model.image_processor.preprocess(canvas, height=height, width=width)\n",
    "    # init_image = init_image.to(dtype=torch.float32)\n",
    "    # mask = model.mask_processor.preprocess(mask_image, height=height, width=width)\n",
    "    # masked_image = init_image * (mask < 0.5)\n",
    "    # mask, masked_image_latents = model.prepare_mask_latents(\n",
    "    #             mask,\n",
    "    #             masked_image,\n",
    "    #             1,\n",
    "    #             height,\n",
    "    #             width,\n",
    "    #             torch.float16,\n",
    "    #             device,\n",
    "    #             torch.manual_seed(666),\n",
    "    #             True,\n",
    "    # )\n",
    "    # init_mask = mask[:1]\n",
    "    # latents = (1 - init_mask) * latents + init_mask * latent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    image = model.vae.decode(latents / model.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = model.image_processor.postprocess(image, output_type='pil')[0]\n",
    "\n",
    "image.save('output/temp/inpainting.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "# pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "pipe.load_lora_weights(lora_file)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "img_url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo_mask_1.png\"\n",
    "\n",
    "image = load_image(img_url).resize((1024, 1024))\n",
    "mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "# prompt = \"a tiger sitting on a park bench\"\n",
    "# prompt = \"1boy olis solo\"\n",
    "prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(3)\n",
    "\n",
    "image = pipe(\n",
    "  prompt=prompt,\n",
    "  image=image,\n",
    "  mask_image=mask_image,\n",
    "  guidance_scale=8.0,\n",
    "  num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "  strength=0.99,  # make sure to use `strength` below 1.0\n",
    "  generator=generator,\n",
    ").images[0]\n",
    "\n",
    "image.save(\"inpainting.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-steps: \n",
    "1. 先生成一个图片\n",
    "2. 按照mask进行裁剪\n",
    "3. 裁剪后把人物分割下来\n",
    "4. 再贴回原来背景\n",
    "\n",
    "注意：\n",
    "1. 输入的背景质量越好，则生成的人物也越好\n",
    "2. mask框不要太窄或太宽\n",
    "3. prompt中要输入环境，否则很可能直接输出大脸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "# 多步搭建完成后的基础效果\n",
    "\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "from utils.anime_seg.anime_seg import AnimeSegmentation, net_names\n",
    "\n",
    "# v1\n",
    "def generate_image(image, mask_image, prompt):\n",
    "    # pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "    pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "    lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "    pipe.load_lora_weights(lora_file)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    # image = load_image(img_url).resize((1024, 1024))\n",
    "    # mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "    # prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "    # prompt = \"1boy, olis, solo, standing on the ground\"\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "\n",
    "    image = pipe(\n",
    "      prompt=prompt,\n",
    "      image=image,\n",
    "      mask_image=mask_image,\n",
    "      guidance_scale=8.0,\n",
    "      num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "      strength=0.99,  # make sure to use `strength` below 1.0\n",
    "      generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "# 2\n",
    "def crop_image(gen_image, mask_image):\n",
    "    image = gen_image#.resize((width, height)) # 1024, 1024\n",
    "    mask = mask_image.convert(\"L\")  # 转换为灰度图像\n",
    "\n",
    "    # 调整掩模的大小与图像相匹配\n",
    "    mask = mask.resize(image.size, Image.ANTIALIAS)\n",
    "\n",
    "    # 转换为NumPy数组\n",
    "    image_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    # 找到掩模中非零像素的位置\n",
    "    nonzero_indices = np.nonzero(mask_array)\n",
    "\n",
    "    # 获取边界框\n",
    "    top = np.min(nonzero_indices[0])\n",
    "    bottom = np.max(nonzero_indices[0])\n",
    "    left = np.min(nonzero_indices[1])\n",
    "    right = np.max(nonzero_indices[1])\n",
    "\n",
    "    box = (left, top, right, bottom)\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped_image, box\n",
    "\n",
    "\n",
    "def get_mask(model, input_img, use_amp=True, s=640):\n",
    "    h0, w0 = h, w = input_img.shape[0], input_img.shape[1]\n",
    "    if h > w:\n",
    "        h, w = s, int(s * w / h)\n",
    "    else:\n",
    "        h, w = int(s * h / w), s\n",
    "    ph, pw = s - h, s - w\n",
    "    tmpImg = np.zeros([s, s, 3], dtype=np.float32)\n",
    "    tmpImg[ph // 2:ph // 2 + h, pw // 2:pw // 2 + w] = cv2.resize(input_img, (w, h)) / 255\n",
    "    tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "    tmpImg = torch.from_numpy(tmpImg).unsqueeze(0).type(torch.FloatTensor).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with amp.autocast():\n",
    "                pred = model(tmpImg)\n",
    "            pred = pred.to(dtype=torch.float32)\n",
    "        else:\n",
    "            pred = model(tmpImg)\n",
    "        pred = pred[0, :, ph // 2:ph // 2 + h, pw // 2:pw // 2 + w]\n",
    "        pred = cv2.resize(pred.cpu().numpy().transpose((1, 2, 0)), (w0, h0))[:, :, np.newaxis]\n",
    "        return pred\n",
    "\n",
    "def matting_process(image_list):\n",
    "    model = AnimeSegmentation.try_load('isnet_is', './utils/anime_seg/isnetis.ckpt', 'cpu', img_size=1024)\n",
    "    model.eval()\n",
    "\n",
    "    matted_image_list = []\n",
    "    for img in image_list:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = get_mask(model, img, use_amp=True, s=1024)\n",
    "\n",
    "        img_person = np.concatenate((mask * img + 1 - mask, mask * 255), axis=2).astype(np.uint8)\n",
    "        img_person = cv2.cvtColor(img_person, cv2.COLOR_RGBA2BGRA) # cv2.COLOR_RGBA2RGB\n",
    "\n",
    "        matted_image_list.append(img_person)\n",
    "\n",
    "    return matted_image_list\n",
    "\n",
    "# load image\n",
    "width, height = 1024, 1024 # bench：1024, 1024 # 959, 539 # 森林：1100, 1024\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/1.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\"\n",
    "image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/5.png\"\n",
    "image = load_image(image_path).resize((width, height))\n",
    "\n",
    "# load mask\n",
    "# a: 输入mask图片方式\n",
    "# mask_url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo_mask_1.png\"\n",
    "# mask_image = load_image(mask_url).resize((width, height))\n",
    "# b: 输入坐标方式\n",
    "mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "mask_image[0:, 300:800] = 255\n",
    "mask_image = Image.fromarray(mask_image)\n",
    "\n",
    "mask_image.save('mask_image.png')\n",
    "\n",
    "prompt = \"1boy, olis, solo, standing in the classroom\"\n",
    "# prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "\n",
    "gen_image = generate_image(image, mask_image, prompt)\n",
    "gen_image = gen_image.resize((width, height))\n",
    "gen_image.save('gen_image.png')\n",
    "cropped_image, box = crop_image(gen_image, mask_image)\n",
    "cropped_image.save('person_cropped_image.png')\n",
    "print(\"box: \", box)\n",
    "\n",
    "cropped_image = np.array(cropped_image)\n",
    "person_image = matting_process([cropped_image])[0]\n",
    "# 去除半透明的边缘\n",
    "threshold_value = 50 # 设置为0属于完全透明\n",
    "person_image[np.where(person_image[:, :, 3] < threshold_value)] = [0, 0, 0, 0]\n",
    "cv2.imwrite('person_segmented_image.png', person_image)\n",
    "\n",
    "person_image = Image.fromarray(person_image)\n",
    "image.paste(person_image, box, mask=person_image.split()[3])\n",
    "\n",
    "image.save(\"final_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2\n",
    "# 12.11日\n",
    "# 测试了生成不同分辨率的结果，可以看到有必要超分一下\n",
    "# 去掉##处的两行：则是默认的1024*1024，可以提高生成质量（即生成图越大，效果越好），但是属于bug的意外产物，不符合生成逻辑，且贴上去只有一半\n",
    "# 不去掉##：则是1024*512，最正规的生图了， 但是人脸崩掉了，还需要优化，生成图的大小，需要截一下\n",
    "\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "from utils.anime_seg.anime_seg import AnimeSegmentation, net_names\n",
    "\n",
    "# 1\n",
    "def generate_image(image, mask_image, prompt):\n",
    "    # pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "    pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "    lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "    pipe.load_lora_weights(lora_file)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    # image = load_image(img_url).resize((1024, 1024))\n",
    "    # mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "    # prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "    # prompt = \"1boy, olis, solo, standing on the ground\"\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "\n",
    "    image = pipe(\n",
    "      prompt=prompt,\n",
    "      image=image,\n",
    "      ##\n",
    "      width=width,\n",
    "      height=height,\n",
    "      \n",
    "      mask_image=mask_image,\n",
    "      guidance_scale=8.0,\n",
    "      num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "      strength=0.99,  # make sure to use `strength` below 1.0\n",
    "      generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "# 2\n",
    "def crop_image(gen_image, mask_image):\n",
    "    image = gen_image#.resize((width, height)) # 1024, 1024\n",
    "    mask = mask_image.convert(\"L\")  # 转换为灰度图像\n",
    "\n",
    "    # 调整掩模的大小与图像相匹配\n",
    "    mask = mask.resize(image.size, Image.ANTIALIAS)\n",
    "\n",
    "    # 转换为NumPy数组\n",
    "    image_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    # 找到掩模中非零像素的位置\n",
    "    nonzero_indices = np.nonzero(mask_array)\n",
    "\n",
    "    # 获取边界框\n",
    "    top = np.min(nonzero_indices[0])\n",
    "    bottom = np.max(nonzero_indices[0])\n",
    "    left = np.min(nonzero_indices[1])\n",
    "    right = np.max(nonzero_indices[1])\n",
    "\n",
    "    box = (left, top, right, bottom)\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped_image, box\n",
    "\n",
    "\n",
    "def get_mask(model, input_img, use_amp=True, s=640):\n",
    "    h0, w0 = h, w = input_img.shape[0], input_img.shape[1]\n",
    "    if h > w:\n",
    "        h, w = s, int(s * w / h)\n",
    "    else:\n",
    "        h, w = int(s * h / w), s\n",
    "    ph, pw = s - h, s - w\n",
    "    tmpImg = np.zeros([s, s, 3], dtype=np.float32)\n",
    "    tmpImg[ph // 2:ph // 2 + h, pw // 2:pw // 2 + w] = cv2.resize(input_img, (w, h)) / 255\n",
    "    tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "    tmpImg = torch.from_numpy(tmpImg).unsqueeze(0).type(torch.FloatTensor).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with amp.autocast():\n",
    "                pred = model(tmpImg)\n",
    "            pred = pred.to(dtype=torch.float32)\n",
    "        else:\n",
    "            pred = model(tmpImg)\n",
    "        pred = pred[0, :, ph // 2:ph // 2 + h, pw // 2:pw // 2 + w]\n",
    "        pred = cv2.resize(pred.cpu().numpy().transpose((1, 2, 0)), (w0, h0))[:, :, np.newaxis]\n",
    "        return pred\n",
    "\n",
    "def matting_process(image_list):\n",
    "    model = AnimeSegmentation.try_load('isnet_is', './utils/anime_seg/isnetis.ckpt', 'cpu', img_size=1024)\n",
    "    model.eval()\n",
    "\n",
    "    matted_image_list = []\n",
    "    for img in image_list:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = get_mask(model, img, use_amp=True, s=1024)\n",
    "\n",
    "        img_person = np.concatenate((mask * img + 1 - mask, mask * 255), axis=2).astype(np.uint8)\n",
    "        img_person = cv2.cvtColor(img_person, cv2.COLOR_RGBA2BGRA) # cv2.COLOR_RGBA2RGB\n",
    "\n",
    "        matted_image_list.append(img_person)\n",
    "\n",
    "    return matted_image_list\n",
    "\n",
    "# load image\n",
    "# width, height = 1024, 512 # bench：1024, 1024 # 959, 539 # 森林：1100, 1024\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/1.png\"\n",
    "image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/5.png\"\n",
    "\n",
    "image = load_image(image_path) # .resize((width, height))\n",
    "width, height = image.size\n",
    "print('width, height', width, height)\n",
    "\n",
    "# load mask\n",
    "# a: 输入mask图片方式\n",
    "# mask_url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo_mask_1.png\"\n",
    "# mask_image = load_image(mask_url).resize((width, height))\n",
    "# b: 输入坐标方式\n",
    "mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "mask_image[0:, 300:700] = 255\n",
    "mask_image = Image.fromarray(mask_image)\n",
    "\n",
    "mask_image.save('mask_image.png')\n",
    "\n",
    "prompt = \"1boy, olis, solo, full body, standing in the green forest\"\n",
    "# prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "\n",
    "gen_image = generate_image(image, mask_image, prompt)\n",
    "# gen_image = gen_image.resize((width, height))\n",
    "gen_image.save('gen_image.png')\n",
    "cropped_image, box = crop_image(gen_image, mask_image)\n",
    "cropped_image.save('person_cropped_image.png')\n",
    "print(\"box: \", box)\n",
    "\n",
    "cropped_image = np.array(cropped_image)\n",
    "person_image = matting_process([cropped_image])[0]\n",
    "# 去除半透明的边缘\n",
    "threshold_value = 50 # 设置为0属于完全透明\n",
    "person_image[np.where(person_image[:, :, 3] < threshold_value)] = [0, 0, 0, 0]\n",
    "cv2.imwrite('person_segmented_image.png', person_image)\n",
    "\n",
    "person_image = Image.fromarray(person_image)\n",
    "image.paste(person_image, box, mask=person_image.split()[3])\n",
    "\n",
    "image.save(\"final_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3\n",
    "# 12.12日\n",
    "# 加入了*2超分，生成后再缩放回原来尺寸\n",
    "\n",
    "\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "from utils.anime_seg.anime_seg import AnimeSegmentation, net_names\n",
    "\n",
    "# 1\n",
    "def generate_image(image, mask_image, prompt, width, height):\n",
    "    # pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "    pipe = AutoPipelineForInpainting.from_pretrained(\"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "    lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "    pipe.load_lora_weights(lora_file)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    # image = load_image(img_url).resize((1024, 1024))\n",
    "    # mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "    # prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "    # prompt = \"1boy, olis, solo, standing on the ground\"\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "\n",
    "    image = pipe(\n",
    "      prompt=prompt,\n",
    "      image=image,\n",
    "      ##\n",
    "      width=width,\n",
    "      height=height,\n",
    "      \n",
    "      mask_image=mask_image,\n",
    "      guidance_scale=8.0,\n",
    "      num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "      strength=0.99,  # make sure to use `strength` below 1.0\n",
    "      generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "# 2\n",
    "def crop_image(gen_image, mask_image):\n",
    "    image = gen_image#.resize((width, height)) # 1024, 1024\n",
    "    mask = mask_image.convert(\"L\")  # 转换为灰度图像\n",
    "\n",
    "    # 调整掩模的大小与图像相匹配\n",
    "    mask = mask.resize(image.size, Image.ANTIALIAS)\n",
    "\n",
    "    # 转换为NumPy数组\n",
    "    image_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    # 找到掩模中非零像素的位置\n",
    "    nonzero_indices = np.nonzero(mask_array)\n",
    "\n",
    "    # 获取边界框\n",
    "    top = np.min(nonzero_indices[0])\n",
    "    bottom = np.max(nonzero_indices[0])\n",
    "    left = np.min(nonzero_indices[1])\n",
    "    right = np.max(nonzero_indices[1])\n",
    "\n",
    "    box = (left, top, right, bottom)\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped_image, box\n",
    "\n",
    "\n",
    "def get_mask(model, input_img, use_amp=True, s=640):\n",
    "    h0, w0 = h, w = input_img.shape[0], input_img.shape[1]\n",
    "    if h > w:\n",
    "        h, w = s, int(s * w / h)\n",
    "    else:\n",
    "        h, w = int(s * h / w), s\n",
    "    ph, pw = s - h, s - w\n",
    "    tmpImg = np.zeros([s, s, 3], dtype=np.float32)\n",
    "    tmpImg[ph // 2:ph // 2 + h, pw // 2:pw // 2 + w] = cv2.resize(input_img, (w, h)) / 255\n",
    "    tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "    tmpImg = torch.from_numpy(tmpImg).unsqueeze(0).type(torch.FloatTensor).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with amp.autocast():\n",
    "                pred = model(tmpImg)\n",
    "            pred = pred.to(dtype=torch.float32)\n",
    "        else:\n",
    "            pred = model(tmpImg)\n",
    "        pred = pred[0, :, ph // 2:ph // 2 + h, pw // 2:pw // 2 + w]\n",
    "        pred = cv2.resize(pred.cpu().numpy().transpose((1, 2, 0)), (w0, h0))[:, :, np.newaxis]\n",
    "        return pred\n",
    "\n",
    "def matting_process(image_list):\n",
    "    model = AnimeSegmentation.try_load('isnet_is', './utils/anime_seg/isnetis.ckpt', 'cpu', img_size=1024)\n",
    "    model.eval()\n",
    "\n",
    "    matted_image_list = []\n",
    "    for img in image_list:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = get_mask(model, img, use_amp=True, s=1024)\n",
    "\n",
    "        img_person = np.concatenate((mask * img + 1 - mask, mask * 255), axis=2).astype(np.uint8)\n",
    "        img_person = cv2.cvtColor(img_person, cv2.COLOR_RGBA2BGRA) # cv2.COLOR_RGBA2RGB\n",
    "\n",
    "        matted_image_list.append(img_person)\n",
    "\n",
    "    return matted_image_list\n",
    "\n",
    "# load image\n",
    "# width, height = 1024, 512 # bench：1024, 1024 # 959, 539 # 森林：1100, 1024\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/1.png\"\n",
    "image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\"\n",
    "# image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/5.png\"\n",
    "\n",
    "image = load_image(image_path) # .resize((width, height))\n",
    "width, height = image.size\n",
    "print('width, height', width, height)\n",
    "\n",
    "# load mask\n",
    "# a: 输入mask图片方式\n",
    "# mask_url = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo_mask_1.png\"\n",
    "# mask_image = load_image(mask_url).resize((width, height))\n",
    "# b: 输入坐标方式\n",
    "mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "mask_image[0:, 300:700] = 255\n",
    "mask_image = Image.fromarray(mask_image)\n",
    "\n",
    "mask_image.save('mask_image.png')\n",
    "\n",
    "prompt = \"1boy, olis, solo, full body, standing in the green forest\"\n",
    "# prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "\n",
    "upscale_factor = 2\n",
    "gen_image = generate_image(image, mask_image, prompt, width*upscale_factor, height*upscale_factor)\n",
    "gen_image = gen_image.resize((width, height))\n",
    "gen_image.save('gen_image.png')\n",
    "cropped_image, box = crop_image(gen_image, mask_image)\n",
    "cropped_image.save('person_cropped_image.png')\n",
    "print(\"box: \", box)\n",
    "\n",
    "cropped_image = np.array(cropped_image)\n",
    "person_image = matting_process([cropped_image])[0]\n",
    "# 去除半透明的边缘\n",
    "threshold_value = 50 # 设置为0属于完全透明\n",
    "person_image[np.where(person_image[:, :, 3] < threshold_value)] = [0, 0, 0, 0]\n",
    "cv2.imwrite('person_segmented_image.png', person_image)\n",
    "\n",
    "person_image = Image.fromarray(person_image)\n",
    "image.paste(person_image, box, mask=person_image.split()[3])\n",
    "\n",
    "image.save(\"final_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4\n",
    "# 12.13日\n",
    "# 加入了自适应截取\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "from utils.anime_seg.anime_seg import AnimeSegmentation, net_names\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "def mask_box2mask_image(height, width, mask_box):\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[mask_box[1]:mask_box[3], mask_box[0]:mask_box[2]] = 255\n",
    "    mask_image = Image.fromarray(mask_image)\n",
    "    return mask_image\n",
    "\n",
    "def crop_image_with_mask_image(gen_image, mask_image):\n",
    "    image = gen_image\n",
    "    mask = mask_image.convert(\"L\")\n",
    "\n",
    "    mask = mask.resize(image.size, Image.ANTIALIAS)\n",
    "\n",
    "    image_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    nonzero_indices = np.nonzero(mask_array)\n",
    "\n",
    "    top = np.min(nonzero_indices[0])\n",
    "    bottom = np.max(nonzero_indices[0])\n",
    "    left = np.min(nonzero_indices[1])\n",
    "    right = np.max(nonzero_indices[1])\n",
    "\n",
    "    box = (left, top, right, bottom)\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped_image, box\n",
    "\n",
    "def crop_image_with_mask_box(image, mask_box):\n",
    "    width, height = image.size\n",
    "\n",
    "    left = max(0, mask_box[0])\n",
    "    top = max(0, mask_box[1])\n",
    "    right = min(width, mask_box[2])\n",
    "    bottom = min(height, mask_box[3])\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "def crop_bg_image(bg_image, mask_box, padding):\n",
    "    width, height = bg_image.size\n",
    "\n",
    "    mask_box_list = list(mask_box)\n",
    "    # cropped_bg_box_list = [\n",
    "    #     max(mask_box_list[0]-padding, 0), \n",
    "    #     max(mask_box_list[1]-padding, 0), \n",
    "    #     min(mask_box_list[2]+padding, width), \n",
    "    #     min(mask_box_list[3]+padding, height)\n",
    "    #     ]\n",
    "    cropped_bg_box_list = [\n",
    "        max(math.ceil((mask_box_list[0]-padding) / 8) * 8, 0), \n",
    "        max(math.ceil((mask_box_list[1]-padding) / 8) * 8, 0), \n",
    "        min(math.ceil((mask_box_list[2]+padding) / 8) * 8, width), \n",
    "        min(math.ceil((mask_box_list[3]+padding) / 8) * 8, height)\n",
    "        ]\n",
    "    cropped_bg_box = tuple(cropped_bg_box_list)\n",
    "\n",
    "    cropped_bg_box_related_list = (np.array(mask_box_list) - np.array([cropped_bg_box_list[0], cropped_bg_box_list[1], cropped_bg_box_list[0], cropped_bg_box_list[1]])).tolist()\n",
    "    cropped_bg_box_related = tuple(cropped_bg_box_related_list)\n",
    "\n",
    "    cropped_bg_image = crop_image_with_mask_box(bg_image, cropped_bg_box)\n",
    "\n",
    "    return cropped_bg_image, cropped_bg_box, cropped_bg_box_related\n",
    "\n",
    "def generate_image(base_model_path, image, mask_image, lora_file, prompt, width, height, seed):\n",
    "    pipe = AutoPipelineForInpainting.from_pretrained(base_model_path, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "    pipe.load_lora_weights(lora_file)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "    image = pipe(\n",
    "      prompt=prompt,\n",
    "      image=image,\n",
    "      ##\n",
    "      width=width,\n",
    "      height=height,\n",
    "      \n",
    "      mask_image=mask_image,\n",
    "      guidance_scale=8.0,\n",
    "      num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "      strength=0.99,  # make sure to use `strength` below 1.0\n",
    "      generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_mask(model, input_img, use_amp=True, s=640):\n",
    "    h0, w0 = h, w = input_img.shape[0], input_img.shape[1]\n",
    "    if h > w:\n",
    "        h, w = s, int(s * w / h)\n",
    "    else:\n",
    "        h, w = int(s * h / w), s\n",
    "    ph, pw = s - h, s - w\n",
    "    tmpImg = np.zeros([s, s, 3], dtype=np.float32)\n",
    "    tmpImg[ph // 2:ph // 2 + h, pw // 2:pw // 2 + w] = cv2.resize(input_img, (w, h)) / 255\n",
    "    tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "    tmpImg = torch.from_numpy(tmpImg).unsqueeze(0).type(torch.FloatTensor).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with amp.autocast():\n",
    "                pred = model(tmpImg)\n",
    "            pred = pred.to(dtype=torch.float32)\n",
    "        else:\n",
    "            pred = model(tmpImg)\n",
    "        pred = pred[0, :, ph // 2:ph // 2 + h, pw // 2:pw // 2 + w]\n",
    "        pred = cv2.resize(pred.cpu().numpy().transpose((1, 2, 0)), (w0, h0))[:, :, np.newaxis]\n",
    "        return pred\n",
    "\n",
    "def matting_process(image_list):\n",
    "    model = AnimeSegmentation.try_load('isnet_is', './utils/anime_seg/isnetis.ckpt', 'cpu', img_size=1024)\n",
    "    model.eval()\n",
    "\n",
    "    matted_image_list = []\n",
    "    for img in image_list:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = get_mask(model, img, use_amp=True, s=1024)\n",
    "\n",
    "        img_person = np.concatenate((mask * img + 1 - mask, mask * 255), axis=2).astype(np.uint8)\n",
    "        img_person = cv2.cvtColor(img_person, cv2.COLOR_RGBA2BGRA) # cv2.COLOR_RGBA2RGB\n",
    "\n",
    "        matted_image_list.append(img_person)\n",
    "\n",
    "    return matted_image_list\n",
    "\n",
    "\n",
    "# 一些参数\n",
    "# 输出图片的目录\n",
    "output_root_dir = \"/dfs/comicai/chenyu.liu/Whomor/output/Inpainting\"\n",
    "\n",
    "# 背景图片地址\n",
    "bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/1.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/5.png\"\n",
    "\n",
    "# mask的box坐标值，分别是left, top, right, bottom，这个需要手动输入，注意一定不要超过背景图片尺寸，否则最后报错ValueError: images do not match\n",
    "mask_box = (150, 100, 300, 512)\n",
    "# mask_box = (300, 0, 700, 512)\n",
    "# mask_box = (300, 0, 800, 1024)\n",
    "\n",
    "# 以mask周围往外延伸padding长度来剪切背景\n",
    "# 1\n",
    "# 小于150则稍微有点崩，一般情况下不用修改\n",
    "# 根据经验，200正好，\n",
    "# 大于300左右则接近输入全图，\n",
    "# 超大，如2000，大于图片长、宽，则与输入全图无差别\n",
    "# 最后实验，padding为mask的短边时，刚好？待定\n",
    "# padding = 200\n",
    "padding = 2000\n",
    "# padding = int(20 * min(mask_box[2] - mask_box[0], mask_box[3] - mask_box[1]))\n",
    "\n",
    "# 基模地址\n",
    "# base_model_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "base_model_path = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "\n",
    "# 要生成人物的lora地址\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# 生成inpainting区域的prompt\n",
    "prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "# prompt = \"1boy, olis, solo, full body, standing in the green forest\"\n",
    "# prompt = \"1boy, olis, solo, full body, standing in the classroom\"\n",
    "\n",
    "# 随机种子\n",
    "seed = 1\n",
    "\n",
    "# 生成时的扩大尺寸的倍数，根据经验，扩大两倍生成再缩放回原来尺寸，可以生成高清人脸，一定程度上解决脸崩问题，不用修改\n",
    "upscale_factor = 2\n",
    "\n",
    "# 像素筛选阈值，因为生成图经过segment扣取人物后，周围会有半透明背景，低于阈值则设置为全透明，不用修改\n",
    "transparency_threshold_value = 50\n",
    "\n",
    "\n",
    "\n",
    "# load bg_image\n",
    "bg_image = load_image(bg_image_path)\n",
    "width, height = bg_image.size\n",
    "print('bg_image width, height: ', width, height)\n",
    "\n",
    "# load mask_box\n",
    "mask_box_area = (mask_box[2] - mask_box[0]) * (mask_box[3] - mask_box[1])\n",
    "if mask_box[0]<0 or mask_box[1]<0 or mask_box[2]>width or mask_box[3]>height:\n",
    "    print(\"The mask box goes beyond the bg image\")\n",
    "if mask_box_area<100:\n",
    "    print(f\"The area of the mask box is {mask_box_area}, it is too small\")\n",
    "print(\"mask_box: \", mask_box)\n",
    "\n",
    "# gen mask_image\n",
    "mask_image = mask_box2mask_image(height, width, mask_box)\n",
    "mask_image.save(os.path.join(output_root_dir, \"mask_image.png\"))\n",
    "\n",
    "# crop bg_image\n",
    "cropped_bg_image, cropped_bg_box, cropped_bg_box_related = crop_bg_image(bg_image, mask_box, padding)\n",
    "c_width, c_height = cropped_bg_image.size\n",
    "print(\"c_width, c_height: \", c_width, c_height)\n",
    "cropped_bg_image.save(os.path.join(output_root_dir, \"cropped_bg_image.png\"))\n",
    "print(\"cropped_bg_box: \", cropped_bg_box)\n",
    "print(\"cropped_bg_box_related: \", cropped_bg_box_related)\n",
    "\n",
    "# crop bg_mask_image\n",
    "cropped_bg_mask_image = crop_image_with_mask_box(mask_image, cropped_bg_box)\n",
    "cropped_bg_mask_image.save(os.path.join(output_root_dir, \"cropped_bg_mask_image.png\"))\n",
    "\n",
    "# generate gen_image\n",
    "gen_image = generate_image(base_model_path, cropped_bg_image, cropped_bg_mask_image, lora_file, prompt, c_width*upscale_factor, c_height*upscale_factor, seed)\n",
    "gen_image = gen_image.resize((c_width, c_height))\n",
    "gen_image.save(os.path.join(output_root_dir, \"gen_image.png\"))\n",
    "\n",
    "# crop gen_image to get person_cropped_image\n",
    "person_cropped_image = crop_image_with_mask_box(gen_image, cropped_bg_box_related)\n",
    "person_cropped_image.save(os.path.join(output_root_dir, \"person_cropped_image.png\"))\n",
    "\n",
    "# segment person_cropped_image to get person_segmented_image\n",
    "person_cropped_image = np.array(person_cropped_image)\n",
    "person_segmented_image = matting_process([person_cropped_image])[0]\n",
    "\n",
    "# remove translucent bg edges\n",
    "person_segmented_image[np.where(person_segmented_image[:, :, 3] < transparency_threshold_value)] = [0, 0, 0, 0]\n",
    "cv2.imwrite(os.path.join(output_root_dir, \"person_segmented_image.png\"), person_segmented_image)\n",
    "\n",
    "# paste person_segmented_image on bg_image according to the mask_box\n",
    "person_segmented_image = Image.fromarray(person_segmented_image)\n",
    "\n",
    "print('xx', person_segmented_image.size, mask_box)\n",
    "bg_image.paste(person_segmented_image, mask_box, mask=person_segmented_image.split()[3])\n",
    "bg_image.save(os.path.join(output_root_dir, \"final_image.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v5\n",
    "# 12.13\n",
    "# 背景裁剪为正方形，并且resize到1024，生成后，再回到原来\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "from utils.anime_seg.anime_seg import AnimeSegmentation, net_names\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "def mask_box2mask_image(height, width, mask_box):\n",
    "    mask_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    mask_image[mask_box[1]:mask_box[3], mask_box[0]:mask_box[2]] = 255\n",
    "    mask_image = Image.fromarray(mask_image)\n",
    "    return mask_image\n",
    "\n",
    "def crop_image_with_mask_image(gen_image, mask_image):\n",
    "    image = gen_image\n",
    "    mask = mask_image.convert(\"L\")\n",
    "\n",
    "    mask = mask.resize(image.size, Image.ANTIALIAS)\n",
    "\n",
    "    image_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    nonzero_indices = np.nonzero(mask_array)\n",
    "\n",
    "    top = np.min(nonzero_indices[0])\n",
    "    bottom = np.max(nonzero_indices[0])\n",
    "    left = np.min(nonzero_indices[1])\n",
    "    right = np.max(nonzero_indices[1])\n",
    "\n",
    "    box = (left, top, right, bottom)\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "\n",
    "    return cropped_image, box\n",
    "\n",
    "def crop_image_with_mask_box(image, mask_box):\n",
    "    width, height = image.size\n",
    "\n",
    "    left = max(0, mask_box[0])\n",
    "    top = max(0, mask_box[1])\n",
    "    right = min(width, mask_box[2])\n",
    "    bottom = min(height, mask_box[3])\n",
    "\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "def crop_bg_image(bg_image, mask_box, padding):\n",
    "    width, height = bg_image.size\n",
    "\n",
    "    mask_box_list = list(mask_box)\n",
    "    cropped_bg_box_list = [\n",
    "        max(math.ceil((mask_box_list[0]-padding) / 8) * 8, 0), \n",
    "        max(math.ceil((mask_box_list[1]-padding) / 8) * 8, 0), \n",
    "        min(math.ceil((mask_box_list[2]+padding) / 8) * 8, width), \n",
    "        min(math.ceil((mask_box_list[3]+padding) / 8) * 8, height)\n",
    "        ]\n",
    "    cropped_bg_box = tuple(cropped_bg_box_list)\n",
    "\n",
    "    cropped_bg_box_related_list = (np.array(mask_box_list) - np.array([cropped_bg_box_list[0], cropped_bg_box_list[1], cropped_bg_box_list[0], cropped_bg_box_list[1]])).tolist()\n",
    "    cropped_bg_box_related = tuple(cropped_bg_box_related_list)\n",
    "\n",
    "    cropped_bg_image = crop_image_with_mask_box(bg_image, cropped_bg_box)\n",
    "\n",
    "    return cropped_bg_box, cropped_bg_box_related, cropped_bg_image\n",
    "\n",
    "def crop_bg_image_square(bg_image, mask_box, padding):\n",
    "    width, height = bg_image.size\n",
    "    left, top, right, bottom = mask_box\n",
    "\n",
    "    box_width, box_width = right - left, bottom - top\n",
    "    cropped_bg_border_length = min(math.ceil((max(box_width, box_width) + 2 * padding) / 8) * 8, min(width, height)) # 能被8整除，则一定可以被后面的2整除\n",
    "\n",
    "    cropped_bg_left = left - (cropped_bg_border_length - box_width) // 2\n",
    "    cropped_bg_left = max(cropped_bg_left, 0)\n",
    "    width_diff = cropped_bg_left - (width - cropped_bg_border_length)\n",
    "    if width_diff > 0:\n",
    "        cropped_bg_left -= width_diff\n",
    "\n",
    "    cropped_bg_top = top - (cropped_bg_border_length - box_width) // 2\n",
    "    cropped_bg_top = max(cropped_bg_top, 0)\n",
    "    height_diff = cropped_bg_top - (height - cropped_bg_border_length)\n",
    "    if height_diff > 0:\n",
    "        cropped_bg_top -= height_diff\n",
    "    cropped_bg_box = (cropped_bg_left, cropped_bg_top, cropped_bg_left + cropped_bg_border_length, cropped_bg_top + cropped_bg_border_length)\n",
    "\n",
    "    cropped_bg_box_related = tuple((np.array(list(mask_box)) - np.array([cropped_bg_left, cropped_bg_top, cropped_bg_left, cropped_bg_top])).tolist())\n",
    "\n",
    "    cropped_bg_image = crop_image_with_mask_box(bg_image, cropped_bg_box)\n",
    "\n",
    "    return cropped_bg_box, cropped_bg_box_related, cropped_bg_image, cropped_bg_border_length\n",
    "\n",
    "def generate_image(base_model_path, image, mask_image, lora_file, prompt, width, height, seed):\n",
    "    pipe = AutoPipelineForInpainting.from_pretrained(base_model_path, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "    pipe.load_lora_weights(lora_file)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "    image = pipe(\n",
    "      prompt=prompt,\n",
    "      image=image,\n",
    "      ##\n",
    "      width=width,\n",
    "      height=height,\n",
    "      \n",
    "      mask_image=mask_image,\n",
    "      guidance_scale=8.0,\n",
    "      num_inference_steps=20,  # steps between 15 and 30 work well for us\n",
    "      strength=0.99,  # make sure to use `strength` below 1.0\n",
    "      generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_mask(model, input_img, use_amp=True, s=640):\n",
    "    h0, w0 = h, w = input_img.shape[0], input_img.shape[1]\n",
    "    if h > w:\n",
    "        h, w = s, int(s * w / h)\n",
    "    else:\n",
    "        h, w = int(s * h / w), s\n",
    "    ph, pw = s - h, s - w\n",
    "    tmpImg = np.zeros([s, s, 3], dtype=np.float32)\n",
    "    tmpImg[ph // 2:ph // 2 + h, pw // 2:pw // 2 + w] = cv2.resize(input_img, (w, h)) / 255\n",
    "    tmpImg = tmpImg.transpose((2, 0, 1))\n",
    "    tmpImg = torch.from_numpy(tmpImg).unsqueeze(0).type(torch.FloatTensor).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        if use_amp:\n",
    "            with amp.autocast():\n",
    "                pred = model(tmpImg)\n",
    "            pred = pred.to(dtype=torch.float32)\n",
    "        else:\n",
    "            pred = model(tmpImg)\n",
    "        pred = pred[0, :, ph // 2:ph // 2 + h, pw // 2:pw // 2 + w]\n",
    "        pred = cv2.resize(pred.cpu().numpy().transpose((1, 2, 0)), (w0, h0))[:, :, np.newaxis]\n",
    "        return pred\n",
    "\n",
    "def matting_process(image_list):\n",
    "    model = AnimeSegmentation.try_load('isnet_is', './utils/anime_seg/isnetis.ckpt', 'cpu', img_size=1024)\n",
    "    model.eval()\n",
    "\n",
    "    matted_image_list = []\n",
    "    for img in image_list:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = get_mask(model, img, use_amp=True, s=1024)\n",
    "\n",
    "        img_person = np.concatenate((mask * img + 1 - mask, mask * 255), axis=2).astype(np.uint8)\n",
    "        img_person = cv2.cvtColor(img_person, cv2.COLOR_RGBA2BGRA) # cv2.COLOR_RGBA2RGB\n",
    "\n",
    "        matted_image_list.append(img_person)\n",
    "\n",
    "    return matted_image_list\n",
    "\n",
    "\n",
    "# 一些参数\n",
    "# 输出图片的目录\n",
    "output_root_dir = \"/dfs/comicai/chenyu.liu/Whomor/output/Inpainting\"\n",
    "\n",
    "# 背景图片地址\n",
    "bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/huggingface_images/overture-creations-5sI6fQgYIuo.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/1.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/4.png\"\n",
    "# bg_image_path = \"/dfs/comicai/chenyu.liu/Whomor/scene_images/5.png\"\n",
    "\n",
    "# mask的box坐标值，分别是left, top, right, bottom，这个需要手动输入，注意一定不要超过背景图片尺寸，否则最后报错ValueError: images do not match\n",
    "mask_box = (150, 50, 400, 450)\n",
    "# mask_box = (300, 0, 700, 512)\n",
    "# mask_box = (300, 0, 800, 1024)\n",
    "\n",
    "# 以mask周围往外延伸padding长度来剪切背景\n",
    "# 1\n",
    "# 小于150则稍微有点崩，一般情况下不用修改\n",
    "# 根据经验，200正好，\n",
    "# 大于300左右则接近输入全图，\n",
    "# 超大，如2000，大于图片长、宽，则与输入全图无差别\n",
    "# 最后实验，padding为mask的短边时，刚好？待定\n",
    "# padding = 200\n",
    "padding = 2000\n",
    "# padding = int(20 * min(mask_box[2] - mask_box[0], mask_box[3] - mask_box[1]))\n",
    "\n",
    "# 基模地址\n",
    "# base_model_path = \"/dfs/comicai/chenyu.liu/Models/diffusers_stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "base_model_path = \"/dfs/comicai/guanyu.zhao/models/CounterfeitXL\"\n",
    "\n",
    "# 要生成人物的lora地址\n",
    "lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311152018/whomor_boy_role_lora_64_Lion_202311152018-000024.safetensors\"\n",
    "\n",
    "# 生成inpainting区域的prompt\n",
    "prompt = \"1boy, olis, solo, sitting on a park bench\"\n",
    "# prompt = \"1boy, olis, solo, full body, standing in the green forest\"\n",
    "# prompt = \"1boy, olis, solo, full body, standing in the classroom\"\n",
    "\n",
    "# 随机种子\n",
    "seed = 0\n",
    "\n",
    "# 当裁剪后的背景图片的边（正方形）小于该阈值时，超分\n",
    "upscale_pixel_threshold_value = 1024\n",
    "# 生成时的扩大尺寸的倍数，根据经验，扩大两倍生成再缩放回原来尺寸，可以生成高清人脸，一定程度上解决脸崩问题，默认为1，不用修改，后续还会根据upscale_pixel_threshold_value再改变\n",
    "upscale_factor = 1\n",
    "\n",
    "# 像素筛选阈值，因为生成图经过segment扣取人物后，周围会有半透明背景，低于阈值则设置为全透明，不用修改\n",
    "transparency_threshold_value = 50\n",
    "\n",
    "\n",
    "# load bg_image\n",
    "bg_image = load_image(bg_image_path)\n",
    "width, height = bg_image.size\n",
    "print('bg_image_width, bg_image_width: ', width, height)\n",
    "\n",
    "# load mask_box\n",
    "mask_box_area = (mask_box[2] - mask_box[0]) * (mask_box[3] - mask_box[1])\n",
    "if mask_box[0] < 0 or mask_box[1] < 0 or mask_box[2] > width or mask_box[3] > height:\n",
    "    print(\"The mask box goes beyond the bg image\")\n",
    "if mask_box_area < 100:\n",
    "    print(f\"The area of the mask box is {mask_box_area}, it is too small\")\n",
    "print(\"mask_box: \", mask_box)\n",
    "\n",
    "# gen mask_image\n",
    "mask_image = mask_box2mask_image(height, width, mask_box)\n",
    "mask_image.save(os.path.join(output_root_dir, \"mask_image.png\"))\n",
    "\n",
    "# crop bg_image\n",
    "cropped_bg_box, cropped_bg_box_related, cropped_bg_image, cropped_bg_border_length = crop_bg_image_square(bg_image, mask_box, padding)\n",
    "cropped_bg_width, cropped_bg_height = cropped_bg_image.size\n",
    "print(\"cropped_bg_width, cropped_bg_height: \", cropped_bg_width, cropped_bg_height)\n",
    "print(\"cropped_bg_border_length: \", cropped_bg_border_length)\n",
    "if cropped_bg_border_length < upscale_pixel_threshold_value:\n",
    "    upscale_factor = 2\n",
    "\n",
    "cropped_bg_image.save(os.path.join(output_root_dir, \"cropped_bg_image.png\"))\n",
    "print(\"cropped_bg_box: \", cropped_bg_box)\n",
    "print(\"cropped_bg_box_related: \", cropped_bg_box_related)\n",
    "\n",
    "# crop bg_mask_image\n",
    "cropped_bg_mask_image = crop_image_with_mask_box(mask_image, cropped_bg_box)\n",
    "cropped_bg_mask_image.save(os.path.join(output_root_dir, \"cropped_bg_mask_image.png\"))\n",
    "\n",
    "# generate gen_image\n",
    "gen_image = generate_image(base_model_path, cropped_bg_image, cropped_bg_mask_image, lora_file, prompt, cropped_bg_width*upscale_factor, cropped_bg_height*upscale_factor, seed)\n",
    "gen_image = gen_image.resize((cropped_bg_width, cropped_bg_height))\n",
    "gen_image.save(os.path.join(output_root_dir, \"gen_image.png\"))\n",
    "\n",
    "# crop gen_image to get person_cropped_image\n",
    "person_cropped_image = crop_image_with_mask_box(gen_image, cropped_bg_box_related)\n",
    "person_cropped_image.save(os.path.join(output_root_dir, \"person_cropped_image.png\"))\n",
    "\n",
    "# segment person_cropped_image to get person_segmented_image\n",
    "person_cropped_image = np.array(person_cropped_image)\n",
    "person_segmented_image = matting_process([person_cropped_image])[0]\n",
    "\n",
    "# remove translucent bg edges\n",
    "person_segmented_image[np.where(person_segmented_image[:, :, 3] < transparency_threshold_value)] = [0, 0, 0, 0]\n",
    "cv2.imwrite(os.path.join(output_root_dir, \"person_segmented_image.png\"), person_segmented_image)\n",
    "\n",
    "# paste person_segmented_image on bg_image according to the mask_box\n",
    "person_segmented_image = Image.fromarray(person_segmented_image)\n",
    "\n",
    "print(\"person_segmented_image.size: \", person_segmented_image.size)\n",
    "print(\"mask_box: \", mask_box)\n",
    "bg_image.paste(person_segmented_image, mask_box, mask=person_segmented_image.split()[3])\n",
    "bg_image.save(os.path.join(output_root_dir, \"final_image.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from_single_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载单个模型文件\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline#, EulerAncestralDiscreteScheduler # DiffusionPipeline, DPMSolverMultistepScheduler,\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# base_model_name = '/dfs/comicai/algo-assemble-platform/models/t2iadapter_models/v1-5-pruned-emaonly.ckpt'\n",
    "# base_model_name = '/dfs/comicai/chenyu.liu/algo-assemble-platform/core/t2iadapter/models/sd-v1-4.ckpt'\n",
    "base_model_name = \"/dfs/comicai/algo-assemble-platform/models/sdxlNijiSpecial_sdxlNijiSE.safetensors\"\n",
    "\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_role_lora_64_Lion_202311091852/whomor_role_lora_64_Lion_202311091852-000024.safetensors\"\n",
    "# lora_file = \"/dfs/comicai/guanyu.zhao/lora-scripts/output/whomor_boy_role_lora_64_Lion_202311141425/whomor_boy_role_lora_64_Lion_202311141425-000028.safetensors\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_name, \n",
    "    # safety_checker=None, \n",
    "    # cache_dir='models', \n",
    "    # local_files_only=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "# pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.load_lora_weights(lora_file)\n",
    "# pipe.to(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单张\n",
    "import cv2\n",
    "\n",
    "# root_path = \"/dfs/comicai/chenyu.liu/Whomor/ori/\"\n",
    "root_path = \"/dfs/comicai/chenyu.liu/Whomor/\"\n",
    "img_name = 'output_train_controlnet_finetune_scribble.png'\n",
    "\n",
    "# 读取图像\n",
    "image = cv2.imread(root_path+img_name, cv2.IMREAD_GRAYSCALE)  # 替换成你的图像文件名\n",
    "\n",
    "# 使用Canny边缘检测\n",
    "edges = cv2.Canny(image, threshold1=100, threshold2=200)  # 调整阈值以获得最佳边缘\n",
    "\n",
    "# 保存边缘检测结果\n",
    "cv2.imwrite(root_path+\"canny_\"+img_name, edges)  # 保存为JPEG格式，也可以根据需要更改文件格式和文件名\n",
    "\n",
    "# # 显示原始图像和边缘检测结果（可选）\n",
    "# cv2.imshow('Original Image', image)\n",
    "# cv2.imshow('Canny Edges', edges)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "ori_path = \"/dfs/comicai/chenyu.liu/Whomor/images_test/03_whomor_color\"\n",
    "tar_path = \"/dfs/comicai/chenyu.liu/Whomor/images_test/03_whomor_color_canny\"\n",
    "\n",
    "dir_or_files = os.listdir(ori_path)\n",
    "dir_or_files = [dir_or_file for dir_or_file in dir_or_files if dir_or_file not in ['.ipynb_checkpoints']]\n",
    "for dir_file in dir_or_files:\n",
    "    image = cv2.imread(os.path.join(ori_path,dir_file), cv2.IMREAD_GRAYSCALE)\n",
    "    edges = cv2.Canny(image, threshold1=100, threshold2=200)\n",
    "    cv2.imwrite(os.path.join(tar_path,dir_file), edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn it into a black line on a white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 但是保存的是双线条，不好用\n",
    "import cv2\n",
    "\n",
    "# 读取彩色图像\n",
    "image = cv2.imread('/dfs/comicai/chenyu.liu/Whomor/images_test/03_whomor_color/1280X1280_1.PNG')\n",
    "\n",
    "# 将彩色图转换成灰度图\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 使用Canny边缘检测算法\n",
    "edges = cv2.Canny(gray_image, 50, 150, apertureSize=3)\n",
    "\n",
    "# 将边缘轮廓置为白色，背景置为黑色\n",
    "inverted_edges = cv2.bitwise_not(edges)\n",
    "\n",
    "# 保存线稿图\n",
    "cv2.imwrite('./image_edges.jpg', inverted_edges)\n",
    "\n",
    "\n",
    "# 2. 通过阈值保存成单线条，但是不好用\n",
    "import cv2\n",
    "\n",
    "# 读取彩色图像\n",
    "image = cv2.imread('/dfs/comicai/chenyu.liu/Whomor/images_test/03_whomor_color/1280X1280_1.PNG')\n",
    "\n",
    "# 将彩色图转换成灰度图\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 应用阈值化来得到单色线稿图像\n",
    "_, thresholded = cv2.threshold(gray_image, 90, 100, cv2.THRESH_BINARY)\n",
    "\n",
    "# 保存线稿图\n",
    "cv2.imwrite('./image_edges.jpg', thresholded)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
